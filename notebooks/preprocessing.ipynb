{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming conventions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.jams**\n",
    "\n",
    "[Guitarist]\\_[Genre][ChordProgression]-[bpm]-[key]\\_[Solo or Chords]\n",
    "\n",
    "Example:\n",
    "\n",
    "00_BN1-129-Eb_solo.jams\n",
    "\n",
    "**.wav**\n",
    "\n",
    "[Guitarist]\\_[Genre][ChordProgression]-[bpm]-[key]\\_[Solo or Chords]\\_[Pickup]\\_[Processing]\n",
    "\n",
    "Example:\n",
    "\n",
    "00_BN1-129-Eb_solo_hex_cln.wav"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Audio**\n",
    "\n",
    "1. Load .wav file\n",
    "1. Resample .wav file from 44kHz to 22 kHz\n",
    "1. Constant-Q Transformatiom\n",
    "1. Extract Frames\n",
    "1. Generate windows with 9 frames width\n",
    "\n",
    "**Labels**\n",
    "\n",
    "1. Load corresponding .jams file\n",
    "1. Extract Midi notes and timestamps\n",
    "1. Generate Labels per window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import jams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../data/output/\"\n",
    "WINDOW_SIZE = 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .wav file and .jams file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example files\n",
    "annot_file = \"00_BN1-129-Eb_solo.jams\"\n",
    "audio_file = \"00_BN1-129-Eb_solo_mic.wav\"\n",
    "\n",
    "# get current working directory\n",
    "swizzle_dir = '/'.join(os.getcwd().split('/')[:-1])\n",
    "annot_dir = swizzle_dir + '/data/raw/annotation/'\n",
    "audio_dir = swizzle_dir + '/data/raw/audio_mono-mic/'\n",
    "\n",
    "# load annotation file and audio file\n",
    "annot = jams.load(annot_dir+annot_file)\n",
    "audio, sr = librosa.load(audio_dir+audio_file, sr=22050)\n",
    "\n",
    "# normalize audio\n",
    "audio = librosa.util.normalize(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio_dir+audio_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract midinotes and timestamps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_midi = pd.DataFrame()\n",
    "\n",
    "# get midi notes for all strings for complete song\n",
    "for idx, i in enumerate(annot['annotations']['note_midi']):\n",
    "    # extract string played\n",
    "    string = [idx] * len(i['data'])\n",
    "\n",
    "    # build temporary table with midi data and string number\n",
    "    temp = pd.concat([pd.DataFrame(string), pd.DataFrame(i['data'])], axis=1)\n",
    "\n",
    "    # update df_midi\n",
    "    df_midi = pd.concat([df_midi, temp], axis=0)\n",
    "    del temp, string\n",
    "\n",
    "# calculate the end_time of a note by adding time and duration\n",
    "df_midi['end_time'] = df_midi['time'] + df_midi['duration']\n",
    "\n",
    "# correct midi notes\n",
    "df_midi['corrected_value'] = np.round(df_midi['value'], 0)\n",
    "df_midi['corrected_value'] = df_midi['corrected_value'].astype('int')\n",
    "\n",
    "\n",
    "# sort dataframe by time and reset the index\n",
    "df_midi = df_midi.sort_values(by='time').reset_index()\n",
    "\n",
    "# drop index and confidence columns\n",
    "df_midi.drop(['confidence', 'index'], axis=1, inplace=True)\n",
    "\n",
    "df_midi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_midi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in conversion table (midi <-> note <-> frequency)\n",
    "# source: https://musicinformationretrieval.com/midi_conversion_table.html\n",
    "df_conv = pd.read_csv('../data/raw/midi_annotations/conversion_table.csv', usecols=['note', 'midi-ET', 'Hertz-ET'])\n",
    "df_conv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_midi with conversion table\n",
    "df_midi = df_midi.merge(df_conv, left_on='corrected_value', right_on='midi-ET', how='left')\n",
    "\n",
    "# drop duplicates (Eb == D#, C# == Db, etc pp)\n",
    "df_midi.drop_duplicates(subset=['time', 'duration', 'value', 'end_time'], keep='last', inplace=True)\n",
    "\n",
    "# rename string column from 0 to 'string'\n",
    "df_midi = df_midi.rename(mapper={0: 'string'}, axis=1)\n",
    "\n",
    "df_midi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# scatter plot: string vs time\n",
    "fig = sns.scatterplot(data=df_midi, x='time', y='string', marker='');\n",
    "\n",
    "# annotations: notes played\n",
    "for i in df_midi.values:\n",
    "    fig.annotate(i[6], xy=(i[1], i[0]-0.04))\n",
    "\n",
    "fig.set_ylim(0, 5);\n",
    "fig.set_yticklabels(['E', 'A', 'D', 'G', 'B', 'e']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate spectrogram from .wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConstantQ transformation\n",
    "# it's a function of amplitude vs log(freq)\n",
    "# hop_length of 512 corresponds to a framerate of 43 fps\n",
    "# with for example 22 seconds, this will result in ~ 950 values\n",
    "deepest_note = 'E2'\n",
    "hop_length = 512\n",
    "\n",
    "audio_cqt = np.abs(librosa.cqt(audio, sr=sr, hop_length=hop_length, n_bins=192, bins_per_octave=24))\n",
    "\n",
    "# Convert amplitude to sound pressure level in decibel (dB)\n",
    "audio_cqt_dB = librosa.amplitude_to_db(audio_cqt, ref=np.max)\n",
    "\n",
    "# Plot the resulting spectrogram (Frequency vs. Time, colorcode: dB)\n",
    "# using specshow with y_axis='log', signals happening in the midrange are better visible\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "img = librosa.display.specshow(audio_cqt_dB, sr=sr, x_axis='time', y_axis='hz', ax=ax) # change y_axis to 'cqt_note' if you want to see the notes\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sliding windows from spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_frame(input, width: int = 9, left: bool = True):\n",
    "    \"\"\"Padding function to account for windows which left or right bounds are\n",
    "    < 0 or > len(input)\n",
    "\n",
    "    Args:\n",
    "        input (list): frame to be padded\n",
    "        width (int, optional): Window width. Defaults to 9.\n",
    "        left (bool, optional): Left or right padding. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: the padded input.\n",
    "    \"\"\"\n",
    "    orig_width = len(input)\n",
    "    padding = width - orig_width\n",
    "\n",
    "    input = list(input)\n",
    "    \n",
    "    if padding == 0:\n",
    "        return np.array(input)\n",
    "        \n",
    "    if left:\n",
    "        input = [0] * padding + input \n",
    "\n",
    "    else:\n",
    "        input = input + [0] * padding\n",
    "\n",
    "    return np.array(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(input, width: int = 9):\n",
    "    \"\"\"Sliding window function to extract windows with set width from an input array\n",
    "\n",
    "    Args:\n",
    "        input (list): Spectrogram\n",
    "        width (int, optional): Window width. Defaults to 9 frames.\n",
    "\n",
    "    Returns:\n",
    "        list: list of windows\n",
    "    \"\"\"\n",
    "    \n",
    "    frames = []\n",
    "    half_width = width//2\n",
    "\n",
    "    # i: different frequency bins\n",
    "    # j: different timepoints\n",
    "\n",
    "    for i in input:\n",
    "    \n",
    "        freq_bin = []\n",
    "    \n",
    "        for j, _ in enumerate(i):\n",
    "            \n",
    "            # set left and right bounds, so that item j is centered\n",
    "            lbound = j-half_width\n",
    "            rbound = j+half_width+1\n",
    "\n",
    "            # if bounds within input indices, just append \n",
    "            if lbound >= 0 and rbound <= len(input[0]):\n",
    "                freq_bin.append(i[lbound:rbound])\n",
    "            \n",
    "            # if left bound below zero, pad left\n",
    "            elif lbound < 0:\n",
    "                freq_bin.append(pad_frame(i[0:rbound], width, True))\n",
    "\n",
    "            # if right bound greater than input length, pad right\n",
    "            elif rbound > len(input[0]):\n",
    "                freq_bin.append(pad_frame(i[lbound:], width, False))\n",
    "                \n",
    "        frames.append(freq_bin)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(input):\n",
    "    \"\"\"wraps the output from extract_frames in one array\n",
    "\n",
    "    Args:\n",
    "        input (list): list of windows\n",
    "    \"\"\"\n",
    "    images = []\n",
    "\n",
    "    for j in range(len(input[0])):\n",
    "        temp = []\n",
    "        for i in input:\n",
    "            temp.append(i[j])\n",
    "\n",
    "        images.append(temp)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = extract_frames(audio_cqt)\n",
    "X = get_windows(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_windows does this:\n",
    "# np.swapaxes(nn_input,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(get_windows(frames)[0], cmap='gray');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract corresponding label data from .jams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and calculate key numbers\n",
    "dur = librosa.get_duration(y=audio)\n",
    "fps = sr // hop_length\n",
    "n_frames = fps * dur\n",
    "n_frames_int = int(np.round(n_frames, 0))\n",
    "\n",
    "# ToDo: label variables with f string\n",
    "print(f\"Duration: {dur:.2f}s\\nFrames  : {n_frames_int}\\nFPS     : {fps}/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = len(get_windows(frames))\n",
    "window_labels = []\n",
    "times = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    lbound = ((i-(9//2))/n_windows) * dur\n",
    "    rbound = ((i+(9//2))/n_windows) * dur\n",
    "    window_labels.append(df_midi[(df_midi['time'] >= lbound) & (df_midi['time'] <= rbound)][['string', 'corrected_value']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique notes played for this song\n",
    "df_midi['corrected_value'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_fret(window_labels):\n",
    "    tuning = [40, 45, 50, 55, 59, 64]\n",
    "    fretboard = np.zeros((len(window_labels), 6, 21))\n",
    "    \n",
    "    # 1: get string played\n",
    "    # 2: for each note played during window,\n",
    "    #    get empty string midi value (esmv)\n",
    "    # 3: subtract esmv from played note midi value\n",
    "    # 3.1: if fret played is 0,\n",
    "    # 4: replace respective value in fretboard\n",
    "    # 5: set first value to 1, if all other values are 0\n",
    "\n",
    "    for widx, window in enumerate(window_labels):\n",
    "        if window.size > 0:\n",
    "            for item in window:\n",
    "                # empty string midi value from string played (0-5)\n",
    "                esmv = tuning[item[0]]\n",
    "                # convert played note to fret\n",
    "                fret = item[1] - esmv + 1\n",
    "                # set fret to 1 in fretboard\n",
    "                fretboard[widx][item[0]][fret] = 1\n",
    "    \n",
    "        # if no note was played in window, set first values to 1\n",
    "        elif window.size == 0: \n",
    "            for idx in range(len(fretboard[widx])):\n",
    "                fretboard[widx][idx][0] = 1 \n",
    "\n",
    "    for widx in range(len(fretboard)):\n",
    "        for idx, string in enumerate(fretboard[widx]):\n",
    "            if sum(string) == 0:\n",
    "                fretboard[widx][idx][0] = 1\n",
    "        \n",
    "    # return fretboard\n",
    "    return fretboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = midi_to_fret(window_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and shape of CNN training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_frames_idx = []\n",
    "data_frames_idx = []\n",
    "\n",
    "# loop through all frames\n",
    "for fidx, frame in enumerate(y):\n",
    "    \n",
    "    null_strings = 0\n",
    "\n",
    "    # loop over all strings in frame\n",
    "    for sidx, string in enumerate(frame):\n",
    "\n",
    "        # read out indices of '1's\n",
    "        zero_idx = np.where(string==1)\n",
    "\n",
    "        # if '1' is at position 0, increase null_string counter\n",
    "        if np.squeeze(zero_idx).size == 1 and np.squeeze(zero_idx) == 0:\n",
    "            null_strings += 1\n",
    "\n",
    "    # check if all strings were not played\n",
    "    if null_strings == 6:\n",
    "        noise_frames_idx.append(fidx)\n",
    "    else:\n",
    "        data_frames_idx.append(fidx)\n",
    "\n",
    "print(f\"In file \\\"{annot_file[:-5]}\\\", {len(noise_frames_idx)}/{len(y)} are empty frames ({np.round(len(noise_frames_idx)/len(y), 2) * 100} %).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.95\n",
    "\n",
    "mask = np.ones(len(y), dtype=bool)\n",
    "noise_frames_idx = noise_frames_idx[:int(len(noise_frames_idx)*fraction)]\n",
    "mask[noise_frames_idx] = False\n",
    "y[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving output of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(data: np.array, labels: np.array):\n",
    "    # filename has no extenstion\n",
    "    filename = audio_file.split('.')[0]\n",
    "    #num_frames = self.load_rep_and_labels_from_raw_file(filename)\n",
    "    #print \"done: \" + filename + \", \" + str(num_frames) + \" frames\" \n",
    "    save_path = OUTPUT_PATH\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    np.savez(save_path + filename + \"_data_notebook.npz\", data)\n",
    "    np.savez(save_path + filename + \"_labels_notebook.npz\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading output of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loaded = np.load(OUTPUT_PATH+audio_file.split('.')[0]+'_data_notebook.npz')\n",
    "y_loaded = np.load(OUTPUT_PATH+audio_file.split('.')[0]+'_labels_notebook.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loaded['arr_0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_loaded['arr_0'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbound = (12-4) * (1/43)\n",
    "rbound = (12+4) * (1/43)\n",
    "\n",
    "print(lbound, rbound)\n",
    "\n",
    "df_midi[(df_midi['time'] < rbound) & (df_midi['time'] >= lbound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded['arr_0'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_labels[12]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing user audio data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User data will be handled a bit differently. While it is good for the CNN to see notes in different contexts for the training (hence the overlapping sliding window), it is more important to have maximum resolution while predicting on user audio.\n",
    "\n",
    "This is why here, we implement a different approach: after the CQT, each frame is made into a 192x9 \"window\". The CNN then predicts on this \"window\" and outputs its predictions, which are now on frame level (i.e. with a resolution of frames per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_user_audio(audio, window_size):\n",
    "\n",
    "    # perform CQT\n",
    "    cqt = np.abs(librosa.cqt(audio, sr=sr, hop_length=hop_length, n_bins=192, bins_per_octave=24))\n",
    "\n",
    "    # swap axes so it's TIME x FREQUENCIES instead of the other way around\n",
    "    cqt_swapped = np.swapaxes(cqt, 0, 1)\n",
    "    \n",
    "    # initialize numpy array\n",
    "    n_freqs, n_frames = cqt.shape\n",
    "    r = np.zeros((n_frames, n_freqs, window_size))\n",
    "\n",
    "    # construct arrays for each frame over all frequency bins\n",
    "    for idx, frame in enumerate(cqt_swapped):\n",
    "        r[idx] = np.swapaxes([frame] * 9, 0, 1)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_windows = pp_user_audio(audio, WINDOW_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test padding function\n",
    "test = [1, 1, 1, 1, 1, 1]\n",
    "assert sum(pad_frame(test, 9, True)) == sum([0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# test frame_windows output\n",
    "# the first item of the first frame_windows array\n",
    "# should match the first item in the fourth frames item\n",
    "assert frames[0][4][0] == frame_windows[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test against test output from pipeline\n",
    "np.round(frame_windows[0][0][0], 4) == np.round(4.93400028*10**-2, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ce22ec9b47a0c8818982fc2e83d14df8db5ffc759b38b6aa7930ef673c52175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
