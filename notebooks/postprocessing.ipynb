{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows our postprocessing pipeline for one example and compares it to the \"ground truth\" (i.e. labels directly extracted from the jams file). Please note: since this only uses one example, the generalized code is actually a bit more complex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to postprocessing will be a list of arrays with shape 6x21. The list length will vary from song to song."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython\n",
    "import jams\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '00_BN1-129-Eb_solo'\n",
    "audiodata, sr = librosa.load('../data/raw/audio_mono-mic/' + filename + '_mic.wav')\n",
    "jamsdata = jams.load('../data/raw/annotation/' + filename + '.jams')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ground truth from jams files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting midi info from jams file\n",
    "def midi_info_from_jams(data: jams.JAMS):\n",
    "    n = []\n",
    "    s = []\n",
    "    for string, i in enumerate(data.annotations['note_midi']):\n",
    "        for j in i['data']:\n",
    "            n.append([string, *j[:-1]])\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding fret column to notes_played_df\n",
    "def find_frets_from_jams(notes_played, strings_used):\n",
    "    tuning = [40, 45, 50, 55, 59, 64]\n",
    "    notes = [n[3] for n in np.round(notes_played, 0)]\n",
    "    fret = []\n",
    "    for idx, i in enumerate(notes):\n",
    "        if strings_used[idx] < 4:\n",
    "            fret.append(int(notes[idx] - (40 + strings_used[idx] * 5)))\n",
    "        elif strings_used[idx] >= 4:\n",
    "            fret.append(int(notes[idx] - (39 + strings_used[idx] * 5)))\n",
    "    \n",
    "    return fret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get midi info\n",
    "notes_played = midi_info_from_jams(jamsdata)\n",
    "\n",
    "# build dataframe\n",
    "ground_truth_df = pd.DataFrame(notes_played, columns=['string', 'time', 'dur', 'midi'])\n",
    "\n",
    "# cleaning midi information\n",
    "ground_truth_df['midi'] = ground_truth_df['midi'].round(0).astype(int)\n",
    "\n",
    "# adding frets\n",
    "ground_truth_df['fret'] = find_frets_from_jams(notes_played, ground_truth_df['string'].values)\n",
    "\n",
    "# reformatting dataframe\n",
    "ground_truth_df = ground_truth_df.sort_values(by='time')\n",
    "ground_truth_df = ground_truth_df.reset_index().drop('index', axis=1)\n",
    "\n",
    "# adding positional information\n",
    "ground_truth_df['pos'] = list(range(ground_truth_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ground truth to frame-level-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 1  # how many frames to consider\n",
    "fps = 43\n",
    "start = 0\n",
    "interval = (resolution/fps)\n",
    "n_frames = int(np.round(librosa.get_duration(y=audiodata, sr=sr) * fps))\n",
    "\n",
    "frame_labels = []\n",
    "\n",
    "for i in range(n_frames):\n",
    "    lbound = i * interval\n",
    "    rbound = (i+1) * interval\n",
    "    labels = ground_truth_df[(ground_truth_df['time'] >= lbound) & (ground_truth_df['time'] <= rbound)][['string', 'fret']].values\n",
    "    if labels.size > 0:\n",
    "        frame_labels.append(labels)\n",
    "\n",
    "frame_labels = np.array(frame_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_level_labels = pd.DataFrame(frame_labels).explode(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_level_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format frame_level_labels\n",
    "if len(frame_level_labels.columns) == 2:\n",
    "    frame_level_labels.rename(mapper={'index': 'pos', 0: 'string'}, axis=1, inplace=True)\n",
    "    frame_level_labels['fret'] = [i[1] for i in frame_level_labels['string'].values]\n",
    "    frame_level_labels['string'] = [i[0] for i in frame_level_labels['string'].values]\n",
    "\n",
    "else:\n",
    "    frame_level_labels.rename(mapper={'index': 'pos', 0: 'string', 1: 'fret'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_level_labels.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build mock CNN output from frame-level-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_6x21(data: np.array):\n",
    "    # data structure\n",
    "    # index 0: position\n",
    "    # index 1: string\n",
    "    # index 2: fret\n",
    "\n",
    "    r = np.zeros((data.shape[0], 6, 21))\n",
    "    # loop over data\n",
    "    for idx, i in enumerate(data):\n",
    "        r[i[0]][i[1]][i[2]+1] = 1\n",
    "\n",
    "    for widx, i in enumerate(r):\n",
    "        for string, j in enumerate(i):\n",
    "            if sum(j) == 0:\n",
    "                r[widx][string][0] = 1\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_output = build_6x21(np.array(frame_level_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tabs from mock CNN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frets(labels):\n",
    "\n",
    "    tabs = []\n",
    "    \n",
    "    # get indices where labeldata is 1\n",
    "    idx = np.where(labels == 1)\n",
    "\n",
    "    for position, window in enumerate(labels):\n",
    "        for string, data in enumerate(window):\n",
    "            idx = np.where(data[1:] == 1)\n",
    "            if idx[0].size > 0:\n",
    "                tabs.append([position, string, np.squeeze(idx[0][0])]) # [-1] is a quickfix for the error found above!\n",
    "\n",
    "    return pd.DataFrame(tabs, columns=['pos', 'string', 'fret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postproc_df = find_frets(mock_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postproc_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results to ground truth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tabs(data, idx_string, idx_fret, idx_pos):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 6))\n",
    "    tab_fig = sns.scatterplot(data=data, x='pos', y='string', marker=\"x\", color='white', ax=ax)\n",
    "\n",
    "    # rename yticks to string labels\n",
    "    tab_fig.axes.set_yticks(list(range(6)))\n",
    "    tab_fig.set_yticklabels(['E', 'A', 'D', 'G', 'B', 'e'])\n",
    "\n",
    "    # remove xticks\n",
    "    tab_fig.set_xticks([])\n",
    "\n",
    "    # relabel x and y\n",
    "    tab_fig.set_ylabel('String', size='large')\n",
    "    tab_fig.set_xlabel(\"\")\n",
    "\n",
    "    # remove spines\n",
    "    sns.despine(top=True, bottom=True, left=False, right=False)\n",
    "\n",
    "    # add grid\n",
    "    tab_fig.grid(which='major', axis='y', linestyle='-', color='gray')\n",
    "\n",
    "    # add frets\n",
    "    for note in data.values:\n",
    "        tab_fig.annotate(text=str(int(note[idx_fret])), xy=(note[idx_pos], note[idx_string]-0.08), size='large', color=\"red\") # can throw a FutureWarning: triggered by numpy. nothing much we can do about it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('../data/raw/audio_mono-mic/' + filename + '_mic.wav')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: Direct extraction from jams file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tabs(ground_truth_df, idx_string=0, idx_pos=5, idx_fret=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: Frame-level extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tabs(frame_level_labels, idx_string=1, idx_fret=2, idx_pos=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: Extraction from mock CNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tabs(postproc_df, idx_string=1, idx_fret=2, idx_pos=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8 (main, Oct 17 2022, 15:15:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ce22ec9b47a0c8818982fc2e83d14df8db5ffc759b38b6aa7930ef673c52175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
