{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CNN__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use CNN(convolutional neural networks) for the task of guitar tablature estimation. The previous work of Andrew Wiggins and Youngmoo Kim showed that CNNs have shown promise for translating guitar audios to tabs, and the use of CNNs has also been explored for various other tasks within music information retrieval such as musical tempo estimation, key classification, singing voice detection, and instrument classification. It is proven that CNN is a powerful tool for the purpose of our study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Import required packages \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __\"Write Python Script\" function__\n",
    "\n",
    "`%%write_and_run image_modeling.py` is the call of the register cell magic from below in 'w' mode (default). It writes the imports at the beginning of the `image_modeling.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Let's make some dark cell magic. Why not!\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "        print(\"Appended to file \", file)\n",
    "    else:\n",
    "        print('Written to file:', file)\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell.format(**globals()))        \n",
    "    get_ipython().run_cell(cell)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Input Shapes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import variables from our file\n",
    "FRAME_HEIGHT = 192\n",
    "FRAME_WIDTH = 9\n",
    "N_CLASSES = 21\n",
    "N_STRINGS = 6\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 8\n",
    "\n",
    "TRAIN_PATH = 'our trainset path.csv'\n",
    "EVAL_PATH = 'our evalset path.csv'\n",
    "TEST_PATH = 'our testset path.csv'\n",
    "\n",
    "#TRAINING_SIZE = !wc -l < flowers_train.csv\n",
    "#TRAINING_STEPS = int(TRAINING_SIZE[0]) // BATCH_SIZE\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard to monitor our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Load Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_songs = pd.read_csv('our data path')\n",
    "\n",
    "#shuffle our data rows\n",
    "data_songs_shuffled = data_songs.sample(frac = 1, random_state=RSEED).reset_index(drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Do train & test split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df,percentage):#percentage as 0. we want to have for our train set\n",
    "    trainset = df.sample(frac = percentage)\n",
    "    testset = df.drop(trainset.index)\n",
    "    return trainset,testset\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this again and split the train set, so we can produce a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(df,percentage):#percentage as 0.anything, we want to have for our train set\n",
    "    trainset = df.sample(frac = percentage)\n",
    "    evalset = df.drop(trainset.index)\n",
    "    return trainset,evalset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert the test and train data into an array, which is the acceptable form of tensorflow and keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(trainset, dtype = 'float32')\n",
    "test_data = np.array(testset, dtype = 'float32')\n",
    "validation_data = np.array(valset, dtype = 'float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#if we can use the normal way we can split with sklearn\n",
    "x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define our target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our target is are the labels in the dataframe. For the cnn we don't have to split the data into X and y we can feed it only a part of\n",
    "#the data as a train set and another part as a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we define our target by splitting the \n",
    "x_train = train_data[:,1:]/255\n",
    "\n",
    "y_train = train_data[:,0]\n",
    "\n",
    "x_test= test_data[:,1:]/255\n",
    "\n",
    "y_test=test_data[:,0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining softmax function\n",
    "\n",
    "def softmax_by_string(t):\n",
    "        sh = K.shape(t)\n",
    "        string_sm = []\n",
    "        for i in range(N_STRINGS):\n",
    "            string_sm.append(K.expand_dims(K.softmax(t[:,i,:]), axis=1))\n",
    "        return K.concatenate(string_sm, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_swizzle_model():       \n",
    "        swizzle_model = tf.keras.swizzle_models.Sequential()\n",
    "        swizzle_model.add(tf.keras.layers.InputLayer(input_shape=[FRAME_HEIGHT, FRAME_WIDTH, 1], name='image'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.25))   \n",
    "        swizzle_model.add(tf.keras.layers.Flatten())\n",
    "        swizzle_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.5))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(126, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(N_CLASSES * N_STRINGS)) # no activation\n",
    "        swizzle_model.add(tf.keras.layers.Reshape((N_CLASSES, N_STRINGS)))\n",
    "        swizzle_model.add(tf.keras.layers.Activation(softmax_by_string))\n",
    "        return swizzle_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model metrics for the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swizzle_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001),metrics =['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train CNN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = swizzle_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate,y_validate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show plots for our loss function and the accurancy\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train - Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results of our swizzle model metrics\n",
    "score = swizzle_model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c883442287411cfd35d895069543a936e2e23cdf2e951a28e4bcbe06352d4147"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
