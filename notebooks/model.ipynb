{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CNN__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use CNN(convolutional neural networks) for the task of guitar tablature estimation. The previous work of Andrew Wiggins and Youngmoo Kim showed that CNNs have shown promise for translating guitar audios to tabs, and the use of CNNs has also been explored for various other tasks within music information retrieval such as musical tempo estimation, key classification, singing voice detection, and instrument classification. It is proven that CNN is a powerful tool for the purpose of our study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Import required packages \n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#various\n",
    "import datetime\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __\"Write Python Script\" function__\n",
    "\n",
    "`%%write_and_run image_modeling.py` is the call of the register cell magic from below in 'w' mode (default). It writes the imports at the beginning of the `image_modeling.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Let's make some dark cell magic. Why not!\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "        print(\"Appended to file \", file)\n",
    "    else:\n",
    "        print('Written to file:', file)\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell.format(**globals()))        \n",
    "    get_ipython().run_cell(cell)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Input Shapes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import variables from our file\n",
    "FRAME_HEIGHT = 192\n",
    "FRAME_WIDTH = 9\n",
    "N_CLASSES = 21\n",
    "N_STRINGS = 6\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 8\n",
    "\n",
    "TRAIN_PATH = 'our trainset path.csv'\n",
    "EVAL_PATH = 'our evalset path.csv'\n",
    "TEST_PATH = 'our testset path.csv'\n",
    "\n",
    "#TRAINING_SIZE = !wc -l < flowers_train.csv\n",
    "#TRAINING_STEPS = int(TRAINING_SIZE[0]) // BATCH_SIZE\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard to monitor our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Load Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_songs = pd.read_csv('our data path')\n",
    "\n",
    "#shuffle our data rows\n",
    "data_songs_shuffled = data_songs.sample(frac = 1, random_state=RSEED).reset_index(drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define our target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our target is are the labels in the dataframe. For the cnn we don't have to split the data into X and y we can feed it only a part of\n",
    "#the data as a train set and another part as a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 'audio file'\n",
    "y = annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Do train & test split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#if we can use the normal way we can split with sklearn\n",
    "x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df,percentage):#percentage as 0. we want to have for our train set\n",
    "    trainset = df.sample(frac = percentage)\n",
    "    testset = df.drop(trainset.index)\n",
    "    return trainset,testset\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this again and split the train set, so we can produce a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(df,percentage):#percentage as 0.anything, we want to have for our train set\n",
    "    trainset = df.sample(frac = percentage)\n",
    "    evalset = df.drop(trainset.index)\n",
    "    return trainset,evalset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert the test and train data into an array, which is the acceptable form of tensorflow and keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(trainset, dtype = 'float32')\n",
    "test_data = np.array(testset, dtype = 'float32')\n",
    "validation_data = np.array(valset, dtype = 'float32')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Reshape__\n",
    "\n",
    "Now we shall build the CNN!\n",
    "\n",
    "Actually, no.\n",
    "\n",
    "We'll have to do one final step of reshaping ALL the data before we go forward to build it.\n",
    "\n",
    "Why do we need to reshape the data? Because each data point is in the form of a single row. In this CNN example, we're going to work with IMAGES.\n",
    "\n",
    "The CNN needs images to extract coarse and fine features so that it can train and classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_shape = (FRAME_HEIGHT, FRAME_WIDTH, 1)\n",
    "x_train = train_data.reshape(train_data.shape[0], *im_shape) # Python TIP :the * operator unpacks the tuple\n",
    "x_test = test_data.reshape(test_data[0], *im_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining softmax function\n",
    "\n",
    "def softmax_by_string(t):\n",
    "        sh = K.shape(t)\n",
    "        string_sm = []\n",
    "        for i in range(N_STRINGS):\n",
    "            string_sm.append(K.expand_dims(K.softmax(t[:,i,:]), axis=1))\n",
    "        return K.concatenate(string_sm, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_swizzle_model():       \n",
    "        swizzle_model = tf.keras.swizzle_models.Sequential()\n",
    "        swizzle_model.add(tf.keras.layers.InputLayer(input_shape=[FRAME_HEIGHT, FRAME_WIDTH, 1], name='image'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.25))   \n",
    "        swizzle_model.add(tf.keras.layers.Flatten())\n",
    "        swizzle_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.5))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(126, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(N_CLASSES * N_STRINGS)) # no activation\n",
    "        swizzle_model.add(tf.keras.layers.Reshape((N_CLASSES, N_STRINGS)))\n",
    "        swizzle_model.add(tf.keras.layers.Activation(softmax_by_string))\n",
    "        return swizzle_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model metrics for the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "#RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) \n",
    "#adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swizzle_model.compile(loss ='sparse_categorical_crossentropy', optimizer= optimizer, metrics =['accuracy'])\n",
    "#we use sparse_categorical_crossentropy because we have a one-hot-encoding for every note. so we have only 0 and 1. If we have more\n",
    "#numbers like this, we have to use categorical_crossentropy.\n",
    "#we use accuracy because we want to have a good overall performance of the model. Besides that for us, every tone \n",
    "#has the same importance so all classes have same importance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use annealer to decrease learning rate in each epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ReduceLROnPlateau function from Keras.callbacks, we choose to reduce the LR by half if the accuracy is not improved after 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train CNN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = swizzle_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validate,y_validate),\n",
    "    callbacks=[learning_rate_reduction],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show plots for our loss function and the accurancy\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train - Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results of our swizzle model metrics\n",
    "score = swizzle_model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swizzle_model.save('cnn_Tabs_of_songs_with_20_epochs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8 (main, Oct 17 2022, 15:26:33) \n[Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c883442287411cfd35d895069543a936e2e23cdf2e951a28e4bcbe06352d4147"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
