{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CNN__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use CNN(convolutional neural networks) for the task of guitar tablature estimation. The previous work of Andrew Wiggins and Youngmoo Kim showed that CNNs have shown promise for translating guitar audios to tabs, and the use of CNNs has also been explored for various other tasks within music information retrieval such as musical tempo estimation, key classification, singing voice detection, and instrument classification. It is proven that CNN is a powerful tool for the purpose of our study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages \n",
    "\n",
    "#various\n",
    "import datetime\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "RSEED = 42\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Input Shapes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of all our constants we use for our model \n",
    "\n",
    "FRAME_HEIGHT = 192\n",
    "FRAME_WIDTH = 9\n",
    "N_CLASSES = 21\n",
    "N_STRINGS = 6\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard to monitor our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Load Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load the data from our output folder from preprocessing\n",
    "INPUT_PATH = \"../data/output/\"\n",
    "save_path = \"../app/model/\"\n",
    "\n",
    "#for all\n",
    "# IMAGES = np.load(INPUT_PATH + 'training_data_all_75.npz')\n",
    "# annots = np.load(INPUT_PATH + 'training_labels_all_75.npz')\n",
    "\n",
    "\n",
    "# for solo  \n",
    "IMAGES = np.load(INPUT_PATH + 'training_data_solo_95.npz')\n",
    "annots = np.load(INPUT_PATH + 'training_labels_solo_95.npz')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Do train & test split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we have to split our dataset into train and test set. We use 70% for the train set and 30% for the test set.\n",
    "train_images, test_images, train_annots, test_annots = train_test_split(IMAGES['arr_0'], annots['arr_0'], test_size= 0.2, shuffle=True, random_state= RSEED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we need also a validation set we split once more. We take this time 10% of the train set for \n",
    "#the validation set and take the rest for training.\n",
    "train_images, validate_images,train_annots,validate_annots = train_test_split(train_images, train_annots, test_size = 0.1, shuffle=True, random_state = RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89510, 192, 9)\n",
      "(24864, 192, 9)\n",
      "(9946, 192, 9)\n",
      "(89510, 6, 21)\n",
      "(24864, 6, 21)\n",
      "(9946, 6, 21)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look on the different shapes of our sets\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(validate_images.shape)\n",
    "print(train_annots.shape)\n",
    "print(test_annots.shape)\n",
    "print(validate_annots.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define our softmax function by string__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_by_string(t):\n",
    "        sh = K.shape(t)\n",
    "        string_sm = []\n",
    "        for i in range(N_STRINGS):\n",
    "            string_sm.append(K.expand_dims(K.softmax(t[:,i,:]), axis=1))\n",
    "        return K.concatenate(string_sm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcross_by_string(target, output):\n",
    "        loss = 0\n",
    "        for i in range(N_STRINGS):\n",
    "            loss += K.categorical_crossentropy(target[:,i,:], output[:,i,:])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_acc(y_true, y_pred):\n",
    "        return K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Building our CNN Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function of our cnn model\n",
    "'''what it takes:\n",
    "- a picture with a certain frame height(192pixel) and a frame width(9 pixel)\n",
    "- only one color channel, therefore as a grayscale image\n",
    "\n",
    "what it deliver:\n",
    "\n",
    "An array with the size 6x21. This is representing the 6 different strings of a guitar and 19 different \n",
    "frets of the guitar. The other 2 of the 21 entries represent, if a string is played or not played.\n",
    "\n",
    "The different layers we used you can easily extract from below.\n",
    "'''\n",
    "\n",
    "def cnn_swizzle_model():       \n",
    "        swizzle_model = tf.keras.Sequential()\n",
    "        swizzle_model.add(tf.keras.layers.InputLayer(input_shape=[FRAME_HEIGHT, FRAME_WIDTH, 1]))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.25))   \n",
    "        swizzle_model.add(tf.keras.layers.Flatten())\n",
    "        swizzle_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.5))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(N_CLASSES * N_STRINGS))\n",
    "        swizzle_model.add(tf.keras.layers.Reshape((N_STRINGS, N_CLASSES)))\n",
    "        swizzle_model.add(tf.keras.layers.Activation(softmax_by_string))\n",
    "        return swizzle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:54:27.340005: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-16 17:54:27.340163: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#this is our swizzle model\n",
    "swizzle_model = cnn_swizzle_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 190, 7, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 188, 5, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 186, 3, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 93, 1, 64)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 93, 1, 64)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5952)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               761984    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 126)               16254     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 6, 21)             0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 6, 21)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 833,982\n",
      "Trainable params: 833,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#let's have a look on the model summary to see the different layers and their shapes\n",
    "#we have 3 dimensions in the beginning, then flatten to 1 Dimension for the dense layers and after them\n",
    "#create the end shape representing the guitar with 6 strings and 21 frets\n",
    "swizzle_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model metrics for the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metric: For our model we will use the accuracy metric, because we want to have o good overall \n",
    "prediction of our model. Besides that, for us every tone has the same importance so all classes\n",
    "have the same importance.\n",
    "\n",
    "Optimizer: As an optimizer we take the adam optimizer, which is fast enough to handle our data \n",
    "in a short time\n",
    "\n",
    "Loss function: For the loss function we used categorical crossentropy because we have multiple classes or labels\n",
    "with soft probabilities like [0.5, 0.3, 0.2] and also have a shape like a one-hot-encoded array.\n",
    "'''\n",
    "\n",
    "metrics = avg_acc\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0)\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "\n",
    "swizzle_model.compile(loss=catcross_by_string, optimizer=optimizer, metrics= metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use annealer to decrease learning rate after given epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "'''\n",
    "With the ReduceLROnPlateau function from Keras.callbacks, \n",
    "we choose to reduce the Learning Rate by half if the accuracy is not improved after 3 epochs.\n",
    "'''\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train CNN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder for model \n",
    "'''This function takes the path of a new folder and create a new one. \n",
    "If the folder already exists, it will pass.'''\n",
    "def my_makedirs(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "my_makedirs('../app/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = tf.keras.callbacks.CSVLogger('../data/model/metrics_solo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:54:28.423640: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-16 17:54:29.137160: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 3.6698 - avg_acc: 0.8494"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:55:02.186019: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 35s 48ms/step - loss: 3.6698 - avg_acc: 0.8494 - val_loss: 2.7275 - val_avg_acc: 0.8731\n",
      "Epoch 2/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 2.7693 - avg_acc: 0.8734 - val_loss: 2.6125 - val_avg_acc: 0.8774\n",
      "Epoch 3/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 2.5413 - avg_acc: 0.8830 - val_loss: 2.2985 - val_avg_acc: 0.8924\n",
      "Epoch 4/50\n",
      "700/700 [==============================] - 35s 51ms/step - loss: 2.3839 - avg_acc: 0.8899 - val_loss: 2.1960 - val_avg_acc: 0.8960\n",
      "Epoch 5/50\n",
      "700/700 [==============================] - 34s 49ms/step - loss: 2.2679 - avg_acc: 0.8952 - val_loss: 2.2407 - val_avg_acc: 0.9001\n",
      "Epoch 6/50\n",
      "700/700 [==============================] - 35s 50ms/step - loss: 2.1742 - avg_acc: 0.8993 - val_loss: 1.9865 - val_avg_acc: 0.9069\n",
      "Epoch 7/50\n",
      "700/700 [==============================] - 34s 49ms/step - loss: 2.1017 - avg_acc: 0.9023 - val_loss: 1.9791 - val_avg_acc: 0.9063\n",
      "Epoch 8/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 2.0361 - avg_acc: 0.9050 - val_loss: 1.9231 - val_avg_acc: 0.9105\n",
      "Epoch 9/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.9869 - avg_acc: 0.9074 - val_loss: 1.8753 - val_avg_acc: 0.9124\n",
      "Epoch 10/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.9342 - avg_acc: 0.9095 - val_loss: 1.8264 - val_avg_acc: 0.9152\n",
      "Epoch 11/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.8874 - avg_acc: 0.9113 - val_loss: 1.8277 - val_avg_acc: 0.9149\n",
      "Epoch 12/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.8497 - avg_acc: 0.9124 - val_loss: 1.7668 - val_avg_acc: 0.9179\n",
      "Epoch 13/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.8190 - avg_acc: 0.9136 - val_loss: 1.7902 - val_avg_acc: 0.9159\n",
      "Epoch 14/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.7924 - avg_acc: 0.9147 - val_loss: 1.7543 - val_avg_acc: 0.9186\n",
      "Epoch 15/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.7574 - avg_acc: 0.9161 - val_loss: 1.6983 - val_avg_acc: 0.9195\n",
      "Epoch 16/50\n",
      "700/700 [==============================] - 35s 50ms/step - loss: 1.7317 - avg_acc: 0.9172 - val_loss: 1.7031 - val_avg_acc: 0.9203\n",
      "Epoch 17/50\n",
      "700/700 [==============================] - 34s 49ms/step - loss: 1.7105 - avg_acc: 0.9180 - val_loss: 1.7253 - val_avg_acc: 0.9207\n",
      "Epoch 18/50\n",
      "700/700 [==============================] - 33s 47ms/step - loss: 1.6831 - avg_acc: 0.9191 - val_loss: 1.6774 - val_avg_acc: 0.9221\n",
      "Epoch 19/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.6608 - avg_acc: 0.9195 - val_loss: 1.6690 - val_avg_acc: 0.9228\n",
      "Epoch 20/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.6435 - avg_acc: 0.9202 - val_loss: 1.7044 - val_avg_acc: 0.9217\n",
      "Epoch 21/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.6214 - avg_acc: 0.9210 - val_loss: 1.6384 - val_avg_acc: 0.9230\n",
      "Epoch 22/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.6058 - avg_acc: 0.9217 - val_loss: 1.6514 - val_avg_acc: 0.9221\n",
      "Epoch 23/50\n",
      "700/700 [==============================] - 35s 50ms/step - loss: 1.5920 - avg_acc: 0.9220 - val_loss: 1.6441 - val_avg_acc: 0.9232\n",
      "Epoch 24/50\n",
      "700/700 [==============================] - 35s 50ms/step - loss: 1.5741 - avg_acc: 0.9231 - val_loss: 1.6149 - val_avg_acc: 0.9247\n",
      "Epoch 25/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.5633 - avg_acc: 0.9233 - val_loss: 1.6342 - val_avg_acc: 0.9235\n",
      "Epoch 26/50\n",
      "700/700 [==============================] - 35s 50ms/step - loss: 1.5453 - avg_acc: 0.9238 - val_loss: 1.6740 - val_avg_acc: 0.9241\n",
      "Epoch 27/50\n",
      "700/700 [==============================] - 34s 49ms/step - loss: 1.5304 - avg_acc: 0.9245 - val_loss: 1.6907 - val_avg_acc: 0.9241\n",
      "Epoch 28/50\n",
      "700/700 [==============================] - 35s 49ms/step - loss: 1.5179 - avg_acc: 0.9248 - val_loss: 1.5786 - val_avg_acc: 0.9260\n",
      "Epoch 29/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.5121 - avg_acc: 0.9250 - val_loss: 1.6884 - val_avg_acc: 0.9224\n",
      "Epoch 30/50\n",
      "700/700 [==============================] - 34s 49ms/step - loss: 1.4992 - avg_acc: 0.9258 - val_loss: 1.5738 - val_avg_acc: 0.9259\n",
      "Epoch 31/50\n",
      "700/700 [==============================] - 35s 49ms/step - loss: 1.4865 - avg_acc: 0.9260 - val_loss: 1.5630 - val_avg_acc: 0.9266\n",
      "Epoch 32/50\n",
      "700/700 [==============================] - 35s 49ms/step - loss: 1.4810 - avg_acc: 0.9260 - val_loss: 1.5611 - val_avg_acc: 0.9272\n",
      "Epoch 33/50\n",
      "700/700 [==============================] - 34s 49ms/step - loss: 1.4626 - avg_acc: 0.9273 - val_loss: 1.6359 - val_avg_acc: 0.9257\n",
      "Epoch 34/50\n",
      "700/700 [==============================] - 33s 47ms/step - loss: 1.4544 - avg_acc: 0.9276 - val_loss: 1.5901 - val_avg_acc: 0.9250\n",
      "Epoch 35/50\n",
      "700/700 [==============================] - 33s 47ms/step - loss: 1.4508 - avg_acc: 0.9274 - val_loss: 1.6243 - val_avg_acc: 0.9255\n",
      "Epoch 36/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.4429 - avg_acc: 0.9276 - val_loss: 1.6267 - val_avg_acc: 0.9260\n",
      "Epoch 37/50\n",
      "700/700 [==============================] - 34s 49ms/step - loss: 1.4286 - avg_acc: 0.9283 - val_loss: 1.5570 - val_avg_acc: 0.9275\n",
      "Epoch 38/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.4224 - avg_acc: 0.9285 - val_loss: 1.5816 - val_avg_acc: 0.9265\n",
      "Epoch 39/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.4124 - avg_acc: 0.9292 - val_loss: 1.6307 - val_avg_acc: 0.9266\n",
      "Epoch 40/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.4047 - avg_acc: 0.9290 - val_loss: 1.6626 - val_avg_acc: 0.9266\n",
      "Epoch 41/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.3920 - avg_acc: 0.9301 - val_loss: 1.6028 - val_avg_acc: 0.9261\n",
      "Epoch 42/50\n",
      "700/700 [==============================] - 33s 47ms/step - loss: 1.3901 - avg_acc: 0.9302 - val_loss: 1.6282 - val_avg_acc: 0.9268\n",
      "Epoch 43/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.3857 - avg_acc: 0.9302 - val_loss: 1.5470 - val_avg_acc: 0.9275\n",
      "Epoch 44/50\n",
      "700/700 [==============================] - 35s 50ms/step - loss: 1.3718 - avg_acc: 0.9304 - val_loss: 1.5769 - val_avg_acc: 0.9276\n",
      "Epoch 45/50\n",
      "700/700 [==============================] - 34s 48ms/step - loss: 1.3703 - avg_acc: 0.9307 - val_loss: 1.5710 - val_avg_acc: 0.9271\n",
      "Epoch 46/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.3661 - avg_acc: 0.9310 - val_loss: 1.6075 - val_avg_acc: 0.9263\n",
      "Epoch 47/50\n",
      "700/700 [==============================] - 33s 48ms/step - loss: 1.3616 - avg_acc: 0.9307 - val_loss: 1.5432 - val_avg_acc: 0.9277\n",
      "Epoch 48/50\n",
      "700/700 [==============================] - 35s 50ms/step - loss: 1.3528 - avg_acc: 0.9314 - val_loss: 1.5332 - val_avg_acc: 0.9275\n",
      "Epoch 49/50\n",
      "489/700 [===================>..........] - ETA: 9s - loss: 1.3395 - avg_acc: 0.9319"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#for the training we fit our model and use the batch size and epochs from our constants\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m swizzle_model\u001b[39m.\u001b[39;49mfit( train_images,\n\u001b[1;32m      3\u001b[0m                              train_annots,\n\u001b[1;32m      4\u001b[0m                              batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m      5\u001b[0m                              epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m      6\u001b[0m                              verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m                              use_multiprocessing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m                              validation_data\u001b[39m=\u001b[39;49m(validate_images,validate_annots),\n\u001b[1;32m      9\u001b[0m                              callbacks\u001b[39m=\u001b[39;49m[csv_logger],\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m swizzle_model_metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/model/metrics_solo.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(swizzle_model_metrics\u001b[39m.\u001b[39mto_markdown())\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/engine/training.py:1389\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1389\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1391\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   hook(batch, logs)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1107\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/utils/tf_utils.py:563\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    561\u001b[0m   \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 563\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/utils/tf_utils.py:557\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    555\u001b[0m   \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 557\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    558\u001b[0m   \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \n\u001b[1;32m   1202\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1188\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1190\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for the training we fit our model and use the batch size and epochs from our constants\n",
    "history = swizzle_model.fit( train_images,\n",
    "                             train_annots,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             epochs=EPOCHS,\n",
    "                             verbose=1,\n",
    "                             use_multiprocessing=True,\n",
    "                             validation_data=(validate_images,validate_annots),\n",
    "                             callbacks=[csv_logger],\n",
    ")\n",
    "\n",
    "swizzle_model_metrics = pd.read_csv('../data/model/metrics_solo.csv')\n",
    "print(swizzle_model_metrics.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m#7900AA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m, color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show plots for our loss function and the accurancy\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss', color= '#7900AA')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color = 'c')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['avg_acc'], label='avg. Accuracy', color = '#7900AA')\n",
    "plt.plot(history.history['val_avg_acc'], label='Validation avg. Accuracy', color = 'c')\n",
    "plt.legend()\n",
    "plt.title('Train - Accuracy')\n",
    "\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "\n",
    "fig.savefig('../data/model/plots_s_mm_95_of.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.2565\n",
      "Test Accuracy : 0.9844\n"
     ]
    }
   ],
   "source": [
    "#print results of our swizzle model metrics for training\n",
    "score = swizzle_model.evaluate(test_images,test_annots,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:03:05.682817: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "#the prediction of our model will show us an array with the strings played in the\n",
    "# belonging frame\n",
    "model_output = swizzle_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision = precision_score(test_annots, model_output, pos_label=\"positive\")\n",
    "#print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 6, 21)\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(294, 6, 21)\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#we can have look on the output arrays. We rounded them to have a better overview. \n",
    "#Thats the first entry with a size of 6 by 21.\n",
    "print(test_annots.shape)\n",
    "print(np.round(test_annots[:1][0],2))\n",
    "print(model_output.shape)\n",
    "print(np.round(model_output[:1][0],2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:03:06.125521: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../app/model/swizzle_model_cof/assets\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "swizzle_model.save('../app/model/swizzle_model_s_mm_95_of')\n",
    "\n",
    "loaded_swizzle_model = keras.models.load_model(\"../app/model/swizzle_model_s_mm_95_of\", compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the model output\n",
    "np.save(\"../app/model/model_output_s_mm_95_of.npy\", model_output, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "#np.load(\"../app/model/model_output.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Try Swizzle-Model out in one Song!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
     ]
    }
   ],
   "source": [
    "loaded_swizzle_model = keras.models.load_model(\"../app/model/swizzle_model_s_mm_95_of\", compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_images_load = np.load('../data/output/00_BN1-129-Eb_solo_mic_data.npz')\n",
    "#f_images = f_images_load['arr_0']\n",
    "#f_true_load = np.load('../data/output/00_BN1-129-Eb_solo_mic_labels.npz')\n",
    "#f_true = f_true_load['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = test_images\n",
    "y_true = test_annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:03:08.948735: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_swizzle_model.predict(X_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 6, 21)\n",
      "<class 'numpy.dtype[float64]'>\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(294, 6, 21)\n",
      "<class 'numpy.dtype[float64]'>\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(y_true.shape)\n",
    "print(type(y_true.dtype))\n",
    "print(y_true[1])\n",
    "print(y_pred.shape)\n",
    "print(type(test_annots.dtype))\n",
    "print(np.round(y_pred[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.load('test_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the list below: 294 entries\n",
      "1: Frames which are NOT empty!\n",
      "0: Frames which are empty\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    156\n",
       "0    138\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in y_pred:\n",
    "    corr_i = np.zeros_like(i)\n",
    "    for sidx, string in enumerate(i):\n",
    "        corr_i[sidx][np.argmax(string)] = 1\n",
    "    \n",
    "    x = np.array_equal(test_array, corr_i)\n",
    "    if x == True:\n",
    "        result.append(0)\n",
    "    else:\n",
    "        result.append(1)\n",
    "print('length of the list below:',len(result),'entries')\n",
    "#print(result)\n",
    "df = pd.DataFrame(result)\n",
    "print('1: Frames which are NOT empty!')\n",
    "print('0: Frames which are empty')\n",
    "df.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 6, 21)\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "#__________________________________________________________#\n",
      "(294, 6, 21)\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score\n",
    "\n",
    "print(y_true.shape)\n",
    "print(y_true[20])\n",
    "print('#__________________________________________________________#')\n",
    "print(y_pred.shape)\n",
    "y_true_ravel = y_true.ravel()\n",
    "\n",
    "# argmax the shizzle out of the swizzle\n",
    "corr_y_pred = np.zeros_like(y_pred)\n",
    "\n",
    "for fidx, frame in enumerate(y_pred):\n",
    "    for sidx, string in enumerate(frame):\n",
    "        corr_y_pred[fidx][sidx][np.argmax(string)] = 1\n",
    "\n",
    "print(corr_y_pred[20])\n",
    "\n",
    "corr_y_pred_ravel = corr_y_pred.ravel()\n",
    "print(corr_y_pred_ravel[:50])\n",
    "print(y_true_ravel[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#__________________________________________________________#\n",
      "Accuracy score: 0.9983803045027535\n",
      "#__________________________________________________________#\n",
      "Precision score: 0.9829931972789115\n",
      "#__________________________________________________________#\n",
      "Recall score: 0.9829931972789115\n",
      "#__________________________________________________________#\n",
      "f1_score: 0.9829931972789115\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_true_ravel, corr_y_pred_ravel)\n",
    "prec = precision_score(y_true_ravel, corr_y_pred_ravel)\n",
    "rec = recall_score(y_true_ravel, corr_y_pred_ravel)\n",
    "f1 = f1_score(y_true_ravel, corr_y_pred_ravel)\n",
    "\n",
    "\n",
    "print('#__________________________________________________________#')\n",
    "print('Accuracy score:', acc)\n",
    "print('#__________________________________________________________#')\n",
    "print('Precision score:', prec)\n",
    "print('#__________________________________________________________#')\n",
    "print('Recall score:', rec)\n",
    "print('#__________________________________________________________#')\n",
    "print('f1_score:', f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data characteristics and Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_characteristics(labels: np.array, verbose: bool = True):\n",
    "    dc = {\n",
    "        'empty_frames': 0,\n",
    "        'single_note_frames': 0,\n",
    "        'multi_note_frames': 0\n",
    "    }\n",
    "\n",
    "    # empty frame\n",
    "    empty = [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.]]\n",
    "\n",
    "    if labels.shape[1:] == (6, 21):\n",
    "\n",
    "        for fidx, frame in enumerate(labels):\n",
    "\n",
    "            if np.all(frame == empty):\n",
    "                dc['empty_frames'] += 1\n",
    "\n",
    "            else:\n",
    "                # get number of notes played by number of strings played\n",
    "                n_notes = 6 - sum([i[0] for i in frame])\n",
    "                if n_notes == 1:\n",
    "                    dc['single_note_frames'] += 1\n",
    "                elif n_notes > 1:\n",
    "                    dc['multi_note_frames'] += 1\n",
    "                else: continue\n",
    "    \n",
    "\n",
    "    if verbose:\n",
    "        print(\"-\"*30)\n",
    "        print(\"|\", \" \"*5, \"Label analysis\", \" \"*5, \"|\")\n",
    "        print(\"-\"*30)\n",
    "\n",
    "        for key, value in dc.items():\n",
    "            if key in ['strings_correct', 'null_correct', 'fully_correct']:\n",
    "                print('-'*30)\n",
    "\n",
    "            print(f\"| {key:<19}: {value:>5} |\")\n",
    "\n",
    "        print(\"-\"*30)\n",
    "    \n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "|       Label analysis       |\n",
      "------------------------------\n",
      "| empty_frames       :   137 |\n",
      "| single_note_frames :   153 |\n",
      "| multi_note_frames  :     4 |\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "_ = data_characteristics(y_true, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(true: np.array, test: np.array, transform_preds: bool = True, verbose: bool = True):\n",
    "    \"\"\"Takes true labels and (transformed) test labels in the (n, 6, 21) shape and performs error analysis.\n",
    "\n",
    "    Args:\n",
    "        true (np.array): True labels. Shape expected (n, 6, 21)\n",
    "        test (np.array): Test labels. Shape expected (n, 6, 21)\n",
    "        transform_preds (bool): If true, transforms prediction probabilities to 0 or 1 using argmax. Defaults to True.\n",
    "        verbose (bool): If true, prints out results. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with error analysis data.\n",
    "    \"\"\"\n",
    "\n",
    "    ea = {\n",
    "    'frets_correct': 0,\n",
    "    'frets_wrong': 0,\n",
    "    'strings_correct': 0,\n",
    "    'strings_wrong': 0,\n",
    "    'null_correct': 0,\n",
    "    'null_wrong': 0,\n",
    "    'null_total': 0,\n",
    "    'fully_correct': 0,\n",
    "    'part_correct': 0,\n",
    "    'fully_wrong': 0,\n",
    "    'total': 0\n",
    "}\n",
    "\n",
    "\n",
    "    # transform predictions to be [0, 1]\n",
    "    if transform_preds:\n",
    "        temp = np.zeros_like(test)\n",
    "        for fidx, frame in enumerate(test):\n",
    "            for sidx, string in enumerate(frame):\n",
    "                temp[fidx][sidx][np.argmax(string)] = 1\n",
    "        \n",
    "        test = temp\n",
    "        del temp\n",
    "\n",
    "\n",
    "    # empty frame\n",
    "    empty = [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.]]\n",
    "\n",
    "\n",
    "    # for all frames in true labels\n",
    "    for idx, frame in enumerate(true):\n",
    "\n",
    "        # if no notes were played\n",
    "        if np.all(frame == empty):\n",
    "\n",
    "            if np.all(frame == test[idx]):\n",
    "                ea['null_correct'] += 1\n",
    "            else:\n",
    "                ea['null_wrong'] += 1\n",
    "\n",
    "            ea['null_total'] += 1\n",
    "\n",
    "        # if a note was played\n",
    "        else:\n",
    "            # strings match\n",
    "            if np.all(frame[:, 0] == test[idx][:, 0]):\n",
    "                ea['strings_correct'] += 1\n",
    "            \n",
    "            else: \n",
    "                ea['strings_wrong'] += 1\n",
    "\n",
    "            # frets match\n",
    "            if np.all(frame[:, 1:] == test[idx][:, 1:]):\n",
    "                ea['frets_correct'] += 1\n",
    "            \n",
    "            # only some of the frets match\n",
    "            elif np.any(frame[:, 1:] == test[idx][:, 1:]):\n",
    "                ea['part_correct'] += 1\n",
    "                ea['frets_wrong'] += 1\n",
    "\n",
    "            # no frets match\n",
    "            elif not np.any(frame[:, 1:] == test[idx][:, 1:]):\n",
    "                ea['frets_wrong'] += 1\n",
    "\n",
    "            # nothing matches\n",
    "            if not np.any(frame == test[idx]):\n",
    "                ea['fully_wrong'] += 1\n",
    "            \n",
    "            # everything matches\n",
    "            if np.all(frame == test[idx]):\n",
    "                ea['fully_correct'] += 1\n",
    "        \n",
    "        # increase frame counter\n",
    "        ea['total'] += 1\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"-\"*26)\n",
    "        print(\"|\", \" \"*3, \"Error analysis\", \" \"*3, \"|\")\n",
    "        print(\"-\"*26)\n",
    "\n",
    "        for key, value in ea.items():\n",
    "            if key in ['strings_correct', 'null_correct', 'fully_correct']:\n",
    "                print('-'*26)\n",
    "\n",
    "            print(f\"| {key:<15}: {value:>5} |\")\n",
    "\n",
    "        print(\"-\"*26)\n",
    "    \n",
    "    return ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "|     Error analysis     |\n",
      "--------------------------\n",
      "| frets_correct  :   141 |\n",
      "| frets_wrong    :    16 |\n",
      "--------------------------\n",
      "| strings_correct:   142 |\n",
      "| strings_wrong  :    15 |\n",
      "--------------------------\n",
      "| null_correct   :   123 |\n",
      "| null_wrong     :    14 |\n",
      "| null_total     :   137 |\n",
      "--------------------------\n",
      "| fully_correct  :   141 |\n",
      "| part_correct   :    16 |\n",
      "| fully_wrong    :     0 |\n",
      "| total          :   294 |\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "errors = error_analysis(y_true, y_pred, transform_preds=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total percentage of right predicted values of all values: 89.79591836734694\n",
      "Percentage of right predicted strings out of strings: 90.44585987261146\n",
      "Percentage of right predicted frets out of frets: 89.80891719745223\n"
     ]
    }
   ],
   "source": [
    "#total percentage of total values\n",
    "a = errors['total']/100\n",
    "t = (errors['fully_correct']+errors['null_correct'])/a\n",
    "print('Total percentage of right predicted values of all values:',t)\n",
    "\n",
    "#strings percentage of all strings\n",
    "strings = errors['strings_correct']+errors['strings_wrong']\n",
    "b = strings/100\n",
    "s = errors['strings_correct']/b\n",
    "print('Percentage of right predicted strings out of strings:',s)\n",
    "#fret percentage of all frets\n",
    "strings = errors['frets_correct']+errors['frets_wrong']\n",
    "c = strings/100\n",
    "f = errors['frets_correct']/c\n",
    "print('Percentage of right predicted frets out of frets:',f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c883442287411cfd35d895069543a936e2e23cdf2e951a28e4bcbe06352d4147"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
