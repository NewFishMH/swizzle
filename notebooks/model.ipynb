{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CNN__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use CNN(convolutional neural networks) for the task of guitar tablature estimation. The previous work of Andrew Wiggins and Youngmoo Kim showed that CNNs have shown promise for translating guitar audios to tabs, and the use of CNNs has also been explored for various other tasks within music information retrieval such as musical tempo estimation, key classification, singing voice detection, and instrument classification. It is proven that CNN is a powerful tool for the purpose of our study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Import required packages \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __\"Write Python Script\" function__\n",
    "\n",
    "`%%write_and_run image_modeling.py` is the call of the register cell magic from below in 'w' mode (default). It writes the imports at the beginning of the `image_modeling.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Let's make some dark cell magic. Why not!\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "        print(\"Appended to file \", file)\n",
    "    else:\n",
    "        print('Written to file:', file)\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell.format(**globals()))        \n",
    "    get_ipython().run_cell(cell)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Input Shapes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import variables from our file\n",
    "Frame_height = 192\n",
    "FRAME_WIDTH = 9\n",
    "N_CLASSES = 21\n",
    "N_STRINGS = 6\n",
    "BATCH_SIZE = 128\n",
    "epochs = 8\n",
    "key = \"c\"\n",
    "data_path=\n",
    "annotation_file = \"annotation\"\n",
    "save_path=\n",
    "TRAIN_PATH = 'flowers_train.csv'\n",
    "EVAL_PATH = 'flowers_eval.csv'\n",
    "TEST_PATH = 'flowers_test.csv'\n",
    "\n",
    "TRAINING_SIZE = !wc -l < flowers_train.csv\n",
    "TRAINING_STEPS = int(TRAINING_SIZE[0]) // BATCH_SIZE\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Load Annotation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_IDs():\n",
    "        \n",
    "    save_folder = save_path + key + \" \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"/\"\n",
    "    if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "    log_file = save_folder + \"log.txt\"\n",
    "\n",
    "    #log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics[\"pp\"] = []\n",
    "    metrics[\"pr\"] = []\n",
    "    metrics[\"pf\"] = []\n",
    "    metrics[\"tp\"] = []\n",
    "    metrics[\"tr\"] = []\n",
    "    metrics[\"tf\"] = []\n",
    "    metrics[\"tdr\"] = []\n",
    "    metrics[\"data\"] = [\"g0\",\"g1\",\"g2\",\"g3\",\"g4\",\"g5\",\"mean\",\"std dev\"]\n",
    "\n",
    "    if key == \"c\":\n",
    "        input_shape = (192, FRAME_WIDTH, 1)\n",
    "    elif key == \"m\":\n",
    "        input_shape = (128, FRAME_WIDTH, 1)\n",
    "    elif key == \"cm\":\n",
    "        input_shape = (320, FRAME_WIDTH, 1)\n",
    "    elif key == \"s\":\n",
    "        input_shape = (1025, FRAME_WIDTH, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_IDs():\n",
    "        csv_file = data_path + annotation_file\n",
    "        list_IDs = list(pd.read_csv(csv_file, header=None)[0])\n",
    "        return list_IDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Do train & test split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(data_split):\n",
    "    #we have do to an easier way of splitting data\n",
    "        '''data_split = data_split \n",
    "        partition = {}\n",
    "        partition[\"training\"] = []\n",
    "        partition[\"validation\"] = []\n",
    "        for ID in list_IDs:\n",
    "            guitarist = int(ID.split(\"_\")[0])\n",
    "            if guitarist == data_split:\n",
    "                partition[\"validation\"].append(ID)\n",
    "            else:\n",
    "                partition[\"training\"].append(ID)\n",
    "                \n",
    "        training_generator = DataGenerator(partition['training'], \n",
    "                                                data_path=data_path, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True,\n",
    "                                                spec_repr=key, \n",
    "                                                con_win_size= FRAME_WIDTH)\n",
    "        \n",
    "        validation_generator = DataGenerator(partition['validation'], \n",
    "                                                data_path=data_path, \n",
    "                                                batch_size=len(partition['validation']), \n",
    "                                                shuffle=False,\n",
    "                                                spec_repr=spec_repr, \n",
    "                                                con_win_size=con_win_size)'''\n",
    "        \n",
    "        split_folder = save_folder + str(data_split) + \"/\"\n",
    "        if not os.path.exists(split_folder):\n",
    "            os.makedirs(split_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model():\n",
    "        with open(log_file,'w') as fh:\n",
    "            fh.write(\"\\nbatch_size: \" + str(BATCH_SIZE))\n",
    "            fh.write(\"\\nepochs: \" + str(epochs))\n",
    "            fh.write(\"\\nkey: \" + str(key))\n",
    "            fh.write(\"\\ndata_path: \" + str(data_path))\n",
    "            fh.write(\"\\nframe_width: \" + str(FRAME_WIDTH))\n",
    "            fh.write(\"\\nannotation_file: \" + str(annotation_file) + \"\\n\")\n",
    "            swizzle_model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "       \n",
    "    def softmax_by_string(t):\n",
    "        sh = K.shape(t)\n",
    "        string_sm = []\n",
    "        for i in range(N_STRINGS):\n",
    "            string_sm.append(K.expand_dims(K.softmax(t[:,i,:]), axis=1))\n",
    "        return K.concatenate(string_sm, axis=1)\n",
    "    \n",
    "    def catcross_by_string(target, output):\n",
    "        loss = 0\n",
    "        for i in range(N_STRINGS):\n",
    "            loss += K.categorical_crossentropy(target[:,i,:], output[:,i,:])\n",
    "        return loss\n",
    "    \n",
    "    def avg_acc(y_true, y_pred):\n",
    "        return K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard to monitor our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a testing function.\n",
    "def test(model):\n",
    "    \n",
    "    test_dataset = image_modeling.load_dataset(TEST_PATH, batch_size=1, training=False)\n",
    "    model.evaluate(test_dataset)\n",
    "\n",
    "# Call the testing function for our model\n",
    "test(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_swizzle_model():       \n",
    "        swizzle_model = tf.keras.swizzle_models.Sequential()\n",
    "        swizzle_model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 3], name='image'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),activation='relu',input_shape=self.input_shape))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.25))   \n",
    "        swizzle_model.add(tf.keras.layers.Flatten())\n",
    "        swizzle_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.5))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(self.num_classes * self.num_strings)) # no activation\n",
    "        swizzle_model.add(tf.keras.layers.Reshape((self.num_strings, self.num_classes)))\n",
    "        swizzle_model.add(tf.keras.layers.Activation(self.softmax_by_string))\n",
    "        swizzle_model = swizzle_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train CNN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "        swizzle_model.fit_generator(generator=training_generator,\n",
    "                    validation_data=None,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use Transfer learning where a model is trained on one task and is then reused for another task. One approach to transfer learning is fine-tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Tab/CNN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A CNN to classify 6 fret-string positions\n",
    "    at the frame level during guitar performance\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, Lambda\n",
    "from keras import backend as K\n",
    "from DataGenerator import DataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from Metrics import *\n",
    "\n",
    "class TabCNN:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 batch_size=128, \n",
    "                 epochs=8,\n",
    "                 con_win_size = 9,\n",
    "                 spec_repr=\"c\",\n",
    "                 data_path=\"../data/spec_repr/\",\n",
    "                 id_file=\"id.csv\",\n",
    "                 save_path=\"saved/\"):   \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.con_win_size = con_win_size\n",
    "        self.spec_repr = spec_repr\n",
    "        self.data_path = data_path\n",
    "        self.id_file = id_file\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.load_IDs()\n",
    "        \n",
    "        self.save_folder = self.save_path + self.spec_repr + \" \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"/\"\n",
    "        if not os.path.exists(self.save_folder):\n",
    "            os.makedirs(self.save_folder)\n",
    "        self.log_file = self.save_folder + \"log.txt\"\n",
    "        \n",
    "        self.metrics = {}\n",
    "        self.metrics[\"pp\"] = []\n",
    "        self.metrics[\"pr\"] = []\n",
    "        self.metrics[\"pf\"] = []\n",
    "        self.metrics[\"tp\"] = []\n",
    "        self.metrics[\"tr\"] = []\n",
    "        self.metrics[\"tf\"] = []\n",
    "        self.metrics[\"tdr\"] = []\n",
    "        self.metrics[\"data\"] = [\"g0\",\"g1\",\"g2\",\"g3\",\"g4\",\"g5\",\"mean\",\"std dev\"]\n",
    "        \n",
    "        if self.spec_repr == \"c\":\n",
    "            self.input_shape = (192, self.con_win_size, 1)\n",
    "        elif self.spec_repr == \"m\":\n",
    "            self.input_shape = (128, self.con_win_size, 1)\n",
    "        elif self.spec_repr == \"cm\":\n",
    "            self.input_shape = (320, self.con_win_size, 1)\n",
    "        elif self.spec_repr == \"s\":\n",
    "            self.input_shape = (1025, self.con_win_size, 1)\n",
    "            \n",
    "        # these probably won't ever change\n",
    "        self.num_classes = 21\n",
    "        self.num_strings = 6\n",
    "\n",
    "    def load_IDs(self):\n",
    "        csv_file = self.data_path + self.id_file\n",
    "        self.list_IDs = list(pd.read_csv(csv_file, header=None)[0])\n",
    "        \n",
    "    def partition_data(self, data_split):\n",
    "        self.data_split = data_split\n",
    "        self.partition = {}\n",
    "        self.partition[\"training\"] = []\n",
    "        self.partition[\"validation\"] = []\n",
    "        for ID in self.list_IDs:\n",
    "            guitarist = int(ID.split(\"_\")[0])\n",
    "            if guitarist == data_split:\n",
    "                self.partition[\"validation\"].append(ID)\n",
    "            else:\n",
    "                self.partition[\"training\"].append(ID)\n",
    "                \n",
    "        self.training_generator = DataGenerator(self.partition['training'], \n",
    "                                                data_path=self.data_path, \n",
    "                                                batch_size=self.batch_size, \n",
    "                                                shuffle=True,\n",
    "                                                spec_repr=self.spec_repr, \n",
    "                                                con_win_size=self.con_win_size)\n",
    "        \n",
    "        self.validation_generator = DataGenerator(self.partition['validation'], \n",
    "                                                data_path=self.data_path, \n",
    "                                                batch_size=len(self.partition['validation']), \n",
    "                                                shuffle=False,\n",
    "                                                spec_repr=self.spec_repr, \n",
    "                                                con_win_size=self.con_win_size)\n",
    "        \n",
    "        self.split_folder = self.save_folder + str(self.data_split) + \"/\"\n",
    "        if not os.path.exists(self.split_folder):\n",
    "            os.makedirs(self.split_folder)\n",
    "                \n",
    "    def log_model(self):\n",
    "        with open(self.log_file,'w') as fh:\n",
    "            fh.write(\"\\nbatch_size: \" + str(self.batch_size))\n",
    "            fh.write(\"\\nepochs: \" + str(self.epochs))\n",
    "            fh.write(\"\\nspec_repr: \" + str(self.spec_repr))\n",
    "            fh.write(\"\\ndata_path: \" + str(self.data_path))\n",
    "            fh.write(\"\\ncon_win_size: \" + str(self.con_win_size))\n",
    "            fh.write(\"\\nid_file: \" + str(self.id_file) + \"\\n\")\n",
    "            self.model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "       \n",
    "    def softmax_by_string(self, t):\n",
    "        sh = K.shape(t)\n",
    "        string_sm = []\n",
    "        for i in range(self.num_strings):\n",
    "            string_sm.append(K.expand_dims(K.softmax(t[:,i,:]), axis=1))\n",
    "        return K.concatenate(string_sm, axis=1)\n",
    "    \n",
    "    def catcross_by_string(self, target, output):\n",
    "        loss = 0\n",
    "        for i in range(self.num_strings):\n",
    "            loss += K.categorical_crossentropy(target[:,i,:], output[:,i,:])\n",
    "        return loss\n",
    "    \n",
    "    def avg_acc(self, y_true, y_pred):\n",
    "        return K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "           \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                             activation='relu',\n",
    "                             input_shape=self.input_shape))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))   \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.num_classes * self.num_strings)) # no activation\n",
    "        model.add(Reshape((self.num_strings, self.num_classes)))\n",
    "        model.add(Activation(self.softmax_by_string))\n",
    "\n",
    "        model.compile(loss=self.catcross_by_string,\n",
    "                      optimizer=keras.optimizers.Adadelta(),\n",
    "                      metrics=[self.avg_acc])\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def train(self):\n",
    "        self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=None,\n",
    "                    epochs=self.epochs,\n",
    "                    verbose=1,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=9)\n",
    "        \n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.split_folder + \"weights.h5\")\n",
    "        \n",
    "    def test(self):\n",
    "        self.X_test, self.y_gt = self.validation_generator[0]\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        \n",
    "    def save_predictions(self):\n",
    "        np.savez(self.split_folder + \"predictions.npz\", y_pred=self.y_pred, y_gt=self.y_gt)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.metrics[\"pp\"].append(pitch_precision(self.y_pred, self.y_gt))\n",
    "        self.metrics[\"pr\"].append(pitch_recall(self.y_pred, self.y_gt))\n",
    "        self.metrics[\"pf\"].append(pitch_f_measure(self.y_pred, self.y_gt))\n",
    "        self.metrics[\"tp\"].append(tab_precision(self.y_pred, self.y_gt))\n",
    "        self.metrics[\"tr\"].append(tab_recall(self.y_pred, self.y_gt))\n",
    "        self.metrics[\"tf\"].append(tab_f_measure(self.y_pred, self.y_gt))\n",
    "        self.metrics[\"tdr\"].append(tab_disamb(self.y_pred, self.y_gt))\n",
    "        \n",
    "    def save_results_csv(self):\n",
    "        output = {}\n",
    "        for key in self.metrics.keys():\n",
    "            if key != \"data\":\n",
    "                vals = self.metrics[key]\n",
    "                mean = np.mean(vals)\n",
    "                std = np.std(vals)\n",
    "                output[key] = vals + [mean, std]\n",
    "        output[\"data\"] =  self.metrics[\"data\"]\n",
    "        df = pd.DataFrame.from_dict(output)\n",
    "        df.to_csv(self.save_folder + \"results.csv\") \n",
    "        \n",
    "##################################\n",
    "########### EXPERIMENT ###########\n",
    "##################################\n",
    "\n",
    "tabcnn = TabCNN()\n",
    "\n",
    "print(\"logging model...\")\n",
    "tabcnn.build_model()\n",
    "tabcnn.log_model()\n",
    "\n",
    "for fold in range(6):\n",
    "    print(\"\\nfold \" + str(fold))\n",
    "    tabcnn.partition_data(fold)\n",
    "    print(\"building model...\")\n",
    "    tabcnn.build_model()\n",
    "    print(\"training...\")\n",
    "    tabcnn.train()\n",
    "    tabcnn.save_weights()\n",
    "    print(\"testing...\")\n",
    "    tabcnn.test()\n",
    "    tabcnn.save_predictions()\n",
    "    print(\"evaluation...\")\n",
    "    tabcnn.evaluate()\n",
    "print(\"saving results...\")\n",
    "tabcnn.save_results_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c883442287411cfd35d895069543a936e2e23cdf2e951a28e4bcbe06352d4147"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
