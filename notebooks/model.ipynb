{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CNN__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use CNN(convolutional neural networks) for the task of guitar tablature estimation. The previous work of Andrew Wiggins and Youngmoo Kim showed that CNNs have shown promise for translating guitar audios to tabs, and the use of CNNs has also been explored for various other tasks within music information retrieval such as musical tempo estimation, key classification, singing voice detection, and instrument classification. It is proven that CNN is a powerful tool for the purpose of our study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages \n",
    "\n",
    "#various\n",
    "import datetime\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "RSEED = 42\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __\"Write Python Script\" function__\n",
    "\n",
    "`%%write_and_run image_modeling.py` is the call of the register cell magic from below in 'w' mode (default). It writes the imports at the beginning of the `image_modeling.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Let\\'s make some dark cell magic. Why not!\\n\\nfrom IPython.core.magic import register_cell_magic\\n\\n@register_cell_magic\\ndef write_and_run(line, cell):\\n    argz = line.split()\\n    file = argz[-1]\\n    mode = \\'w\\'\\n    if len(argz) == 2 and argz[0] == \\'-a\\':\\n        mode = \\'a\\'\\n        print(\"Appended to file \", file)\\n    else:\\n        print(\\'Written to file:\\', file)\\n    with open(file, mode) as f:\\n        f.write(cell.format(**globals()))        \\n    get_ipython().run_cell(cell)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Let's make some dark cell magic. Why not!\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "        print(\"Appended to file \", file)\n",
    "    else:\n",
    "        print('Written to file:', file)\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell.format(**globals()))        \n",
    "    get_ipython().run_cell(cell)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Input Shapes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of all our constants we use for our model \n",
    "\n",
    "FRAME_HEIGHT = 192\n",
    "FRAME_WIDTH = 9\n",
    "N_CLASSES = 21\n",
    "N_STRINGS = 6\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "\n",
    "TRAIN_PATH = 'our trainset path.csv'\n",
    "VAL_PATH = 'our evalset path.csv'\n",
    "TEST_PATH = 'our testset path.csv'\n",
    "\n",
    "#TRAINING_SIZE = !wc -l < flowers_train.csv\n",
    "#TRAINING_STEPS = int(TRAINING_SIZE[0]) // BATCH_SIZE\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorboard to monitor our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Load Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load the data from our output folder from preprocessing\n",
    "OUTPUT_PATH = \"../data/output/\"\n",
    "audio_file = \"02_BN1-129-Eb_solo_hex_cln.wav\"\n",
    "\n",
    "    #IMAGES = np.load(OUTPUT_PATH+'/'+audio_file.split('.')[0]+'_data.npz')\n",
    "    #annots = np.load(OUTPUT_PATH+'/'+audio_file.split('.')[0]+'_labels.npz')\n",
    "\n",
    "    #for all\n",
    "IMAGES = np.load('/Users/florianherr/neuefische/capstone_project/swizzle/data/output/training_data.npz')\n",
    "annots = np.load('/Users/florianherr/neuefische/capstone_project/swizzle/data/output/training_labels.npz')\n",
    "\n",
    "    #for solo\n",
    "    #IMAGES = np.load('/Users/florianherr/neuefische/capstone_project/swizzle/data/output/training_datasolo.npz')\n",
    "    #annots = np.load('/Users/florianherr/neuefische/capstone_project/swizzle/data/output/training_labelssolo.npz')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 50000x50000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show some spectrograms of our data frames\n",
    "plt.figure(figsize=(500,500))\n",
    "for i in range(60,63):\n",
    "    plt.subplot(100,100,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(IMAGES['arr_0'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#show the belonging annotations for this three images\n",
    "print(annots['arr_0'][60:63])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Do train & test split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we have to split our dataset into train and test set. We use 70% for the train set and 30% for the test set.\n",
    "train_images, test_images, train_annots, test_annots = train_test_split(IMAGES['arr_0'], annots['arr_0'], test_size= 0.3, random_state= RSEED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we need also a validation set we split once more. We take this time 10% of the train set for \n",
    "#the validation set and take the rest for training.\n",
    "train_images, validate_images,train_annots,validate_annots = train_test_split(train_images,train_annots, test_size = 0.1,random_state = RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297712, 192, 9)\n",
      "(141768, 192, 9)\n",
      "(33080, 192, 9)\n",
      "(297712, 6, 21)\n",
      "(141768, 6, 21)\n",
      "(33080, 6, 21)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look on the different shapes of our sets\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(validate_images.shape)\n",
    "print(train_annots.shape)\n",
    "print(test_annots.shape)\n",
    "print(validate_annots.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define our categorical_crossentropy function by string__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcross_by_string(target, output):\n",
    "    '''The normal categorical_crossentropy was not working with our model, so we had to use a \n",
    "    categorical crossentropy which is working string by string.\n",
    "    It takes:\n",
    "    It returns: Our loss function'''\n",
    "    loss = 0\n",
    "    for i in range(N_STRINGS):\n",
    "        loss += K.categorical_crossentropy(target[:,i,:], output[:,i,:])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_by_string(t):\n",
    "        sh = K.shape(t)\n",
    "        string_sm = []\n",
    "        for i in range(N_STRINGS):\n",
    "            string_sm.append(K.expand_dims(K.softmax(t[:,i,:]), axis=1))\n",
    "        return K.concatenate(string_sm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Building our CNN Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function of our cnn model\n",
    "'''what it takes:\n",
    "- a picture with a certain frame height(192pixel) and a frame width(9 pixel)\n",
    "- only one color channel, therefore as a grayscale image\n",
    "\n",
    "what it deliver:\n",
    "\n",
    "An array with the size 6x21. This is representing the 6 different strings of a guitar and 19 different \n",
    "frets of the guitar. The other 2 of the 21 entries represent, if a string is played or not played.\n",
    "\n",
    "The different layers we used you can easily extract from below.\n",
    "'''\n",
    "\n",
    "def cnn_swizzle_model():       \n",
    "        swizzle_model = tf.keras.Sequential()\n",
    "        swizzle_model.add(tf.keras.layers.InputLayer(input_shape=[FRAME_HEIGHT, FRAME_WIDTH, 1]))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.25))   \n",
    "        swizzle_model.add(tf.keras.layers.Flatten())\n",
    "        swizzle_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dropout(0.5))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(126, activation='relu'))\n",
    "        swizzle_model.add(tf.keras.layers.Dense(N_CLASSES * N_STRINGS)) # no activation\n",
    "        swizzle_model.add(tf.keras.layers.Reshape((N_STRINGS, N_CLASSES)))\n",
    "        swizzle_model.add(tf.keras.layers.Activation('softmax'))\n",
    "        return swizzle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our swizzle model\n",
    "swizzle_model = cnn_swizzle_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 190, 7, 32)        320       \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 188, 5, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 186, 3, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 93, 1, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 93, 1, 64)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 5952)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               761984    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 126)               16254     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 126)               16002     \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 6, 21)             0         \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 6, 21)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 849,984\n",
      "Trainable params: 849,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#let's have a look on the model summary to see the different layers and their shapes\n",
    "#we have 3 dimensions in the beginning, then flatten to 1 Dimension for the dense layers and after them\n",
    "#create the end shape representing the guitar with 6 strings and 21 frets\n",
    "swizzle_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model metrics for the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metric: For our model we will use the accuracy metric, because we want to have o good overall \n",
    "prediction of our model. Besides that, for us every tone has the same importance so all classes\n",
    "have the same importance.\n",
    "\n",
    "Optimizer: As an optimizer we take the adam optimizer, which is fast enough to handle our data \n",
    "in a short time\n",
    "\n",
    "Loss function: For the loss function we used categorical crossentropy because we have multiple classes or labels\n",
    "with soft probabilities like [0.5, 0.3, 0.2] and also have a shape like a one-hot-encoded array.\n",
    "'''\n",
    "metrics =['accuracy']\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#we keep this for later, to test out.\n",
    "#RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) \n",
    "#adadelta\n",
    "optimizer2 = tf.keras.optimizers.Adadelta()\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "\n",
    "loss2 = catcross_by_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swizzle_model.compile(loss=loss2, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use annealer to decrease learning rate after given epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "'''\n",
    "With the ReduceLROnPlateau function from Keras.callbacks, \n",
    "we choose to reduce the Learning Rate by half if the accuracy is not improved after 3 epochs.\n",
    "'''\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Train CNN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder for model \n",
    "'''This function takes the path of a new folder and create a new one. \n",
    "If the folder already exists, it will pass.'''\n",
    "def my_makedirs(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "my_makedirs('../data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('../data/model/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'swizzle_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#for the training we fit our model and use the batch size and epochs from our constants\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m swizzle_model\u001b[39m.\u001b[39mfit( train_images,\n\u001b[1;32m      3\u001b[0m                              train_annots,\n\u001b[1;32m      4\u001b[0m                              batch_size\u001b[39m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      5\u001b[0m                              epochs\u001b[39m=\u001b[39mEPOCHS,\n\u001b[1;32m      6\u001b[0m                              verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m                              use_multiprocessing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                              validation_data\u001b[39m=\u001b[39m(validate_images,validate_annots),\n\u001b[1;32m      9\u001b[0m                              callbacks\u001b[39m=\u001b[39m[learning_rate_reduction,csv_logger],\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m swizzle_model_metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/model/metrics.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(swizzle_model_metrics\u001b[39m.\u001b[39mto_markdown())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'swizzle_model' is not defined"
     ]
    }
   ],
   "source": [
    "#for the training we fit our model and use the batch size and epochs from our constants\n",
    "history = swizzle_model.fit( train_images,\n",
    "                             train_annots,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             epochs=EPOCHS,\n",
    "                             verbose=1,\n",
    "                             use_multiprocessing=True,\n",
    "                             validation_data=(validate_images,validate_annots),\n",
    "                             callbacks=[learning_rate_reduction,csv_logger],\n",
    ")\n",
    "\n",
    "swizzle_model_metrics = pd.read_csv('../data/model/metrics.csv')\n",
    "print(swizzle_model_metrics.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#show plots for our loss function and the accurancy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m      5\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m#7900AA\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#show plots for our loss function and the accurancy\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss', color= '#7900AA')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color = 'c')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy', color = '#7900AA')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color = 'c')\n",
    "plt.legend()\n",
    "plt.title('Train - Accuracy')\n",
    "\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.1933\n",
      "Test Accuracy : 0.9381\n"
     ]
    }
   ],
   "source": [
    "#print results of our swizzle model metrics for training\n",
    "score = swizzle_model.evaluate(test_images,test_annots,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 15:04:30.062563: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "#the prediction of our model will show us an array with the strings played in the\n",
    "# belonging frame\n",
    "output = swizzle_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141768, 6, 21)\n",
      "[[1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. ]\n",
      " [0.9 0.  0.  0.  0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. ]\n",
      " [1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. ]\n",
      " [1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. ]\n",
      " [1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. ]\n",
      " [1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "#we can have look on the output arrays. We rounded them to have a better overview. \n",
    "#Thats the first entry with a size of 6 by 21.\n",
    "print(output.shape)\n",
    "print(np.round(output[:1][0],2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/model/swizzle_model/assets\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown loss function: catcross_by_string. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Save the entire model as a SavedModel.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m swizzle_model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m../data/model/swizzle_model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m loaded_swizzle_model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39m../data/model/swizzle_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/neuefische/capstone_project/swizzle/.venv/lib/python3.9/site-packages/keras/utils/generic_utils.py:709\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    707\u001b[0m   obj \u001b[39m=\u001b[39m module_objects\u001b[39m.\u001b[39mget(object_name)\n\u001b[1;32m    708\u001b[0m   \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 709\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    710\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnknown \u001b[39m\u001b[39m{\u001b[39;00mprintable_module_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mobject_name\u001b[39m}\u001b[39;00m\u001b[39m. Please ensure \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    711\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mthis object is passed to the `custom_objects` argument. See \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    712\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m#registering_the_custom_object for details.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    715\u001b[0m \u001b[39m# Classes passed by name are instantiated with no args, functions are\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# returned as-is.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39mif\u001b[39;00m tf_inspect\u001b[39m.\u001b[39misclass(obj):\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown loss function: catcross_by_string. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "swizzle_model.save('../data/model/swizzle_model')\n",
    "\n",
    "loaded_swizzle_model = keras.models.load_model(\"../data/model/swizzle_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the model output\n",
    "np.save(\"../data/model/model_output.npy\", output, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "#np.load(\"../data/model/model_output.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# confusion matrix\\nimport seaborn as sns\\n\\n# Predict the values from the validation dataset\\nY_pred = swizzle_model.predict(test_images)\\n# Convert predictions classes to one hot vectors \\nY_pred_classes = np.argmax(Y_pred,axis = 1) \\n# Convert validation observations to one hot vectors\\nY_true = np.argmax(test_annots,axis = 1) \\n# compute the confusion matrix\\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \\n# plot the confusion matrix\\nf,ax = plt.subplots(figsize=(8, 8))\\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= \\'.1f\\',ax=ax)\\nplt.xlabel(\"Predicted Label\")\\nplt.ylabel(\"True Label\")\\nplt.title(\"Confusion Matrix\")\\nplt.show()'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = swizzle_model.predict(test_images)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(test_annots,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# when you want to retrieve the model: load json and create model\\njson_file = open(\\'model.json\\', \\'r\\')\\nsaved_model = json_file.read()\\n# close the file as good practice\\njson_file.close()\\nmodel_from_json = model_from_json(saved_model)\\n# load weights into new model\\nmodel_from_json.load_weights(\"model.h5\")\\nprint(\"Model loaded\")'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "swizzle_model_json = swizzle_model.to_json()\n",
    "\n",
    "with open(\"../data/model/swizzle_model.json\", \"w\") as json_file:\n",
    "    json_file.write(swizzle_model_json)\n",
    "\n",
    "# save weights to HDF5\n",
    "swizzle_model.save_weights(\"../data/model/swizzle_model.h5\")\n",
    "print(\"Model saved\")\n",
    "\n",
    "'''# when you want to retrieve the model: load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "saved_model = json_file.read()\n",
    "# close the file as good practice\n",
    "json_file.close()\n",
    "model_from_json = model_from_json(saved_model)\n",
    "# load weights into new model\n",
    "model_from_json.load_weights(\"model.h5\")\n",
    "print(\"Model loaded\")'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Gridsearch for weight initialisation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594\n",
    "\n",
    "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
    "                           batch_size=batch_size, verbose=1)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
    "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(x_train, y_train)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8 (main, Oct 18 2022, 17:01:55) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed92ed0fba541c862e86ee21ecee011cf22ecee6c420718af9366c491a88b484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
