{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration will be done using the **librosa** library for audio, and the **jams** library for annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jams\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = dict()\n",
    "\n",
    "root = '../data/raw/'\n",
    "\n",
    "for dir in os.listdir(root):\n",
    "    size = 0\n",
    "    if dir != '.DS_Store':\n",
    "        for f in os.listdir(root+dir):\n",
    "            size += os.path.getsize(root+dir+'/'+f)\n",
    "\n",
    "        sizes[dir] = np.round(size / 1e6, 2)\n",
    "\n",
    "for key, val in sizes.items():\n",
    "    print(f\"Folder {key} is {val} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation files: *.jams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading *.jams files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotations are saved in *\\*.jams* files. See [here](http://marl.smusic.nyu.edu/papers/humphrey_jams_ismir2014.pdf) for documentation/paper and [here](https://github.com/marl/jams) for the repo.\n",
    "\n",
    "Here, we load all frequencies and midi notes played during a song, with their associated string and save the data in two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of file names in folder\n",
    "def load_file_names(dir: str = '../data/raw/annotation/') -> list:\n",
    "    # create empty list\n",
    "    filenames = []\n",
    "\n",
    "    # go through directory and append path+file name to list\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(dir+f)\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# loading jams file for song\n",
    "def load_jams_file(f: str or list) -> jams.core.JAMS:\n",
    "    # return jams structure\n",
    "    return jams.load(f)\n",
    "\n",
    "\n",
    "# E2–A2–D3–G3–B3–E4\n",
    "stringMap = {0: 'E', 1: 'A', 2: 'D', 3: 'G', 4: 'H', 5: 'e'}\n",
    "\n",
    "# accessing single pickups:\n",
    "def load_time_freq_data(j: jams.JAMS, namespace: str = 'pitch_contour') -> pd.DataFrame:\n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # load all played frequencies and timings for all 6 strings\n",
    "    for i in range(6):\n",
    "        data = pd.DataFrame(j.annotations[namespace][i])\n",
    "        if data.columns.size > 0:\n",
    "            freqs = pd.json_normalize(data['value'])\n",
    "            strings = pd.Series([i] * freqs.shape[0], name='string')\n",
    "            st_name = pd.Series([stringMap[i] for i in strings], name='string_name')\n",
    "            temp = pd.concat([data.iloc[:, 0], strings, st_name, freqs.iloc[:, -1]], axis=1)\n",
    "            df = pd.concat([df, temp], axis=0)\n",
    "\n",
    "    df = df.sort_values(by='time')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_time_midi_data(j: jams.JAMS, namespace: str = 'note_midi') -> pd.DataFrame:\n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # load all played midi notes and timings for all 6 strings\n",
    "    for i in range(6):\n",
    "        data = pd.DataFrame(j.annotations[namespace][i])\n",
    "        if data.columns.size > 0:\n",
    "            strings = pd.Series([i] * data.shape[0], name='string')\n",
    "            st_name = pd.Series([stringMap[i] for i in strings], name='string_name')\n",
    "            temp = pd.concat([data.iloc[:, :-2], strings, st_name, data.iloc[:, -2]], axis=1)\n",
    "            df = pd.concat([df, temp], axis=0)\n",
    "    \n",
    "    df['value'] = np.round(df['value'], 0)\n",
    "    df = df.sort_values(by='time')\n",
    "    df = df.reset_index(drop=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(load_file_names())\n",
    "n_file = 1\n",
    "jams_file = load_jams_file(files[n_file])\n",
    "song_df = load_time_freq_data(jams_file)\n",
    "midi_df = load_time_midi_data(jams_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequencies are more often recorded than midi notes as evident by the dataframe shapes. This is probably because midi notes are recorded only at the onset of a note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.shape, midi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Frequencies over time {song_df.shape}\")\n",
    "display(song_df.head(10))\n",
    "print(f\"Midi notes over time {midi_df.shape}\")\n",
    "display(midi_df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What other data are in the \\*.jams files?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "data = json.load(open(files[n_file], 'rb'))\n",
    "pprint.pprint(data, depth=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available namespaces in JAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jams.list_namespaces()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many annotations do we have? How many are compositions (backing tracks) and how many are solos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_annot = len(files)\n",
    "n_solo = 0\n",
    "n_comp = 0\n",
    "n_c = 0\n",
    "for f in files:\n",
    "    if 'solo' in f: n_solo += 1\n",
    "    elif 'comp' in f: n_comp += 1\n",
    "    if '-C_' in f: n_c +=1\n",
    "\n",
    "print(f\"There are {n_annot} annotation files, split into {n_solo} solos and {n_comp} backing tracks. They are distributed over 5 different genres:\")\n",
    "print(\"Bossa Nova, Funk, Jazz, Rock, Singer Songwriter\")\n",
    "print(f\"{n_c/2} songs are in C\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing annotation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# displaying frequencies played on high e string over time\n",
    "sns.scatterplot(data=song_df, x='time', y='frequency', hue='string');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# displaying frequencies played on high e string over time\n",
    "sns.scatterplot(data=midi_df, x='time', y='value', hue='string');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing and visualizing corresponding audio data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further information see [here](https://www.kdnuggets.com/2020/02/audio-data-analysis-deep-learning-python-part-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../data/raw/audio_hex-pickup_debleeded/' + files[n_file].split('/')[-1].split('.')[0] + '_hex_cln.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listening to the file\n",
    "IPython.display.Audio(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing audio file as floating point time series (x) and sample rate (sr)\n",
    "# setting sr to 'None' preserves native sampling rate of the file\n",
    "x, sr = librosa.load(path=file, sr=None)\n",
    "\n",
    "print(f\"There are {x.shape[0]} points in the audio file with a sample rate of {sr/1000:.2f} kHz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the waveform in the time domain\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "librosa.display.waveshow(x, sr=sr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform short-term fourier-transformation (stft) on x\n",
    "# This is so we know the amplitude of any given frequency\n",
    "X = librosa.stft(x)\n",
    "\n",
    "# Convert amplitude to sound pressure level in decibel (dB)\n",
    "XdB = librosa.amplitude_to_db(abs(X))\n",
    "\n",
    "# Plot the resulting spectrogram (Frequency vs. Time, colorcode: dB)\n",
    "# using specshow with y_axis='log', signals happening in the midrange are better visible\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "img = librosa.display.specshow(XdB, sr=sr, x_axis='time', y_axis='log')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConstantQ transformation is also possible and sometimes better\n",
    "# it plots amplitude vs log(freq)\n",
    "X = np.abs(librosa.cqt(x, sr=sr, hop_length=512, n_bins=192, bins_per_octave=24))\n",
    "\n",
    "# Convert amplitude to sound pressure level in decibel (dB)\n",
    "XdB = librosa.amplitude_to_db(X, ref=np.max)\n",
    "\n",
    "# Plot the resulting spectrogram (Frequency vs. Time, colorcode: dB)\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "img = librosa.display.specshow(XdB, sr=sr, x_axis='time', y_axis='hz', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ce22ec9b47a0c8818982fc2e83d14df8db5ffc759b38b6aa7930ef673c52175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
